# Prometheus Alert Rules for Sixty Sales Dashboard\n# Production-ready alerts with appropriate thresholds\n\ngroups:\n  # High Priority - Immediate Response Required\n  - name: critical_alerts\n    rules:\n      # Service availability\n      - alert: ServiceDown\n        expr: up == 0\n        for: 1m\n        labels:\n          severity: critical\n          team: devops\n        annotations:\n          summary: \"Service {{ $labels.job }} is down\"\n          description: \"Service {{ $labels.job }} on {{ $labels.instance }} has been down for more than 1 minute.\"\n          runbook_url: \"https://wiki.yourdomain.com/runbooks/service-down\"\n\n      # API health check failures\n      - alert: APIHealthCheckFailing\n        expr: probe_success{job=\"blackbox-http\", instance=~\".*api.*\"} == 0\n        for: 2m\n        labels:\n          severity: critical\n          team: backend\n        annotations:\n          summary: \"API health check failing for {{ $labels.instance }}\"\n          description: \"API health check has been failing for {{ $labels.instance }} for more than 2 minutes.\"\n\n      # Frontend availability\n      - alert: FrontendDown\n        expr: probe_success{job=\"blackbox-http\", instance=~\"https://yourdomain.com.*\"} == 0\n        for: 2m\n        labels:\n          severity: critical\n          team: frontend\n        annotations:\n          summary: \"Frontend is down\"\n          description: \"Frontend health check has been failing for more than 2 minutes.\"\n\n      # Database connectivity\n      - alert: DatabaseDown\n        expr: pg_up == 0\n        for: 1m\n        labels:\n          severity: critical\n          team: database\n        annotations:\n          summary: \"Database is down\"\n          description: \"PostgreSQL database is unreachable for more than 1 minute.\"\n\n      # Redis connectivity\n      - alert: RedisDown\n        expr: redis_up == 0\n        for: 1m\n        labels:\n          severity: critical\n          team: backend\n        annotations:\n          summary: \"Redis is down\"\n          description: \"Redis server is unreachable for more than 1 minute.\"\n\n      # SSL Certificate expiring soon\n      - alert: SSLCertificateExpiringSoon\n        expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 7  # 7 days\n        for: 1h\n        labels:\n          severity: critical\n          team: devops\n        annotations:\n          summary: \"SSL certificate expiring soon for {{ $labels.instance }}\"\n          description: \"SSL certificate for {{ $labels.instance }} expires in less than 7 days.\"\n\n  # High Priority - Performance Issues\n  - name: performance_alerts\n    rules:\n      # High response time\n      - alert: HighResponseTime\n        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2\n        for: 5m\n        labels:\n          severity: warning\n          team: backend\n        annotations:\n          summary: \"High response time detected\"\n          description: \"95th percentile response time is {{ $value }}s for more than 5 minutes.\"\n\n      # High error rate\n      - alert: HighErrorRate\n        expr: rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m]) > 0.05\n        for: 3m\n        labels:\n          severity: warning\n          team: backend\n        annotations:\n          summary: \"High error rate detected\"\n          description: \"Error rate is {{ $value | humanizePercentage }} for more than 3 minutes.\"\n\n      # Database query performance\n      - alert: SlowDatabaseQueries\n        expr: rate(pg_stat_database_tup_returned[5m]) / rate(pg_stat_database_tup_fetched[5m]) < 0.8\n        for: 10m\n        labels:\n          severity: warning\n          team: database\n        annotations:\n          summary: \"Database queries are slow\"\n          description: \"Database query efficiency is {{ $value | humanizePercentage }}.\"\n\n      # Memory usage\n      - alert: HighMemoryUsage\n        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.85\n        for: 5m\n        labels:\n          severity: warning\n          team: devops\n        annotations:\n          summary: \"High memory usage on {{ $labels.instance }}\"\n          description: \"Memory usage is {{ $value | humanizePercentage }} for more than 5 minutes.\"\n\n      # CPU usage\n      - alert: HighCPUUsage\n        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 80\n        for: 10m\n        labels:\n          severity: warning\n          team: devops\n        annotations:\n          summary: \"High CPU usage on {{ $labels.instance }}\"\n          description: \"CPU usage is {{ $value }}% for more than 10 minutes.\"\n\n      # Disk space\n      - alert: LowDiskSpace\n        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.1\n        for: 5m\n        labels:\n          severity: warning\n          team: devops\n        annotations:\n          summary: \"Low disk space on {{ $labels.instance }}\"\n          description: \"Disk space is {{ $value | humanizePercentage }} available on {{ $labels.mountpoint }}.\"\n\n  # Medium Priority - Application Specific\n  - name: application_alerts\n    rules:\n      # High number of active users (scaling alert)\n      - alert: HighConcurrentUsers\n        expr: active_users_total > 1000\n        for: 5m\n        labels:\n          severity: info\n          team: backend\n        annotations:\n          summary: \"High number of concurrent users\"\n          description: \"{{ $value }} concurrent users detected. Consider scaling.\"\n\n      # Queue depth (if using job queues)\n      - alert: HighQueueDepth\n        expr: queue_depth > 100\n        for: 10m\n        labels:\n          severity: warning\n          team: backend\n        annotations:\n          summary: \"High queue depth\"\n          description: \"Queue depth is {{ $value }} jobs. Processing may be delayed.\"\n\n      # Cache hit ratio\n      - alert: LowCacheHitRatio\n        expr: redis_cache_hit_ratio < 0.8\n        for: 15m\n        labels:\n          severity: info\n          team: backend\n        annotations:\n          summary: \"Low cache hit ratio\"\n          description: \"Cache hit ratio is {{ $value | humanizePercentage }}. Consider cache optimization.\"\n\n      # Database connections\n      - alert: HighDatabaseConnections\n        expr: pg_stat_database_numbackends / pg_settings_max_connections > 0.8\n        for: 10m\n        labels:\n          severity: warning\n          team: database\n        annotations:\n          summary: \"High database connection usage\"\n          description: \"Database connection usage is {{ $value | humanizePercentage }}.\"\n\n      # Failed login attempts (security)\n      - alert: HighFailedLogins\n        expr: rate(failed_login_attempts_total[5m]) > 10\n        for: 2m\n        labels:\n          severity: warning\n          team: security\n        annotations:\n          summary: \"High number of failed login attempts\"\n          description: \"{{ $value }} failed login attempts per second detected.\"\n\n  # Low Priority - Maintenance and Optimization\n  - name: maintenance_alerts\n    rules:\n      # Container restart frequency\n      - alert: FrequentContainerRestarts\n        expr: rate(container_start_time_seconds[1h]) > 0.1\n        for: 30m\n        labels:\n          severity: info\n          team: devops\n        annotations:\n          summary: \"Container restarting frequently\"\n          description: \"Container {{ $labels.name }} is restarting {{ $value }} times per hour.\"\n\n      # Network errors\n      - alert: NetworkErrors\n        expr: rate(node_network_receive_errs_total[5m]) > 0.01\n        for: 15m\n        labels:\n          severity: info\n          team: devops\n        annotations:\n          summary: \"Network errors detected\"\n          description: \"Network interface {{ $labels.device }} has {{ $value }} errors per second.\"\n\n      # Log volume increase\n      - alert: HighLogVolume\n        expr: rate(log_entries_total[5m]) > (rate(log_entries_total[1h] offset 1d) * 2)\n        for: 10m\n        labels:\n          severity: info\n          team: devops\n        annotations:\n          summary: \"Log volume increased significantly\"\n          description: \"Log volume is {{ $value }}x higher than usual.\"\n\n      # Backup failures\n      - alert: BackupFailed\n        expr: time() - last_successful_backup_timestamp > 86400 * 2  # 2 days\n        for: 1h\n        labels:\n          severity: warning\n          team: devops\n        annotations:\n          summary: \"Backup has not completed successfully recently\"\n          description: \"Last successful backup was {{ $value | humanizeDuration }} ago.\"\n\n  # External Dependencies\n  - name: external_services\n    rules:\n      # Supabase API latency\n      - alert: SupabaseHighLatency\n        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"api-backend\", route=~\".*supabase.*\"}[5m])) > 1\n        for: 10m\n        labels:\n          severity: warning\n          team: backend\n        annotations:\n          summary: \"High latency to Supabase API\"\n          description: \"95th percentile latency to Supabase is {{ $value }}s.\"\n\n      # Third-party service availability\n      - alert: ThirdPartyServiceDown\n        expr: probe_success{job=\"blackbox-http\", instance=~\".*third-party.*\"} == 0\n        for: 5m\n        labels:\n          severity: info\n          team: backend\n        annotations:\n          summary: \"Third-party service unavailable\"\n          description: \"Third-party service {{ $labels.instance }} is unreachable.\"\n\n# Alert routing and notification configuration\n# This would typically be in alertmanager.yml\n# inhibit_rules:\n#   - source_match:\n#       severity: 'critical'\n#     target_match:\n#       severity: 'warning'\n#     equal: ['instance']\n\n# route:\n#   group_by: ['alertname', 'instance']\n#   group_wait: 10s\n#   group_interval: 10s\n#   repeat_interval: 12h\n#   receiver: 'web.hook'\n#   routes:\n#   - match:\n#       severity: critical\n#     receiver: 'pager-duty'\n#   - match:\n#       team: security\n#     receiver: 'security-team'"
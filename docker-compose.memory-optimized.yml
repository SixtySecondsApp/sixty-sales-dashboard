version: '3.8'

services:
  # Frontend - Memory Optimized Nginx
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
      args:
        - NODE_ENV=production
        - VITE_BUILD_MODE=production
    image: sixty-sales-dashboard:production
    container_name: sixty-frontend-optimized
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080" # Monitoring port
    volumes:
      # SSL certificates
      - ./ssl:/etc/ssl/certs:ro
      # Nginx cache volumes (memory-mapped)
      - nginx-cache-api:/var/cache/nginx/api
      - nginx-cache-static:/var/cache/nginx/static
      # Logs for monitoring
      - ./logs/nginx:/var/log/nginx
    environment:
      - NGINX_WORKER_PROCESSES=auto
      - NGINX_WORKER_CONNECTIONS=2048  # Optimized for memory
      - NGINX_WORKER_RLIMIT_NOFILE=4096
    deploy:
      resources:
        limits:
          memory: 128M  # Ultra-lean Nginx configuration
          cpus: '0.5'
        reservations:
          memory: 64M
          cpus: '0.25'
      restart_policy:
        condition: unless-stopped
        delay: 5s
        max_attempts: 3
        window: 120s
    networks:
      - frontend-network
      - monitoring-network
    depends_on:
      - api-backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # API Backend - Memory Constrained
  api-backend:
    build:
      context: .
      dockerfile: Dockerfile
      target: api-backend
      args:
        - NODE_ENV=production
    image: sixty-sales-api:production
    container_name: sixty-api-optimized
    ports:
      - "8000:8000"
      - "9090:9090"  # Metrics port
    volumes:
      - ./logs/api:/app/logs
      - ./backups:/app/backups
    environment:
      - NODE_ENV=production
      - PORT=8000
      # Strict memory management
      - NODE_OPTIONS=--max-old-space-size=384 --gc-interval=50 --optimize-for-size
      - UV_THREADPOOL_SIZE=2
      # Connection pooling (reduced limits)
      - DB_POOL_MIN=1
      - DB_POOL_MAX=5
      - DB_POOL_IDLE_TIMEOUT=10000
      - DB_POOL_ACQUIRE_TIMEOUT=30000
      - DB_POOL_CREATE_TIMEOUT=15000
      # Reduced cache settings
      - CACHE_TTL=1800
      - CACHE_MAX_SIZE=50
      # Rate limiting
      - RATE_LIMIT_WINDOW=900000
      - RATE_LIMIT_MAX=50
      # Monitoring
      - ENABLE_METRICS=true
      - METRICS_PORT=9090
      # Memory monitoring
      - MEMORY_LIMIT_MB=384
      - GC_FREQUENCY=high
      # Database optimizations
      - DATABASE_URL=${DATABASE_URL}
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY}
      - JWT_SECRET=${JWT_SECRET}
      - REDIS_URL=redis://redis:6379
    deploy:
      resources:
        limits:
          memory: 384M  # Strict memory limit
          cpus: '0.75'
        reservations:
          memory: 192M
          cpus: '0.5'
      restart_policy:
        condition: unless-stopped
        delay: 10s
        max_attempts: 3
        window: 120s
    networks:
      - backend-network
      - monitoring-network
    depends_on:
      - redis
      - postgres
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Redis - Ultra Memory Efficient
  redis:
    image: redis:7.2-alpine
    container_name: sixty-redis-optimized
    command: >
      redis-server
      --maxmemory 64mb
      --maxmemory-policy allkeys-lru
      --save 900 1 300 10 60 1000
      --appendonly yes
      --appendfsync everysec
      --auto-aof-rewrite-percentage 100
      --auto-aof-rewrite-min-size 32mb
      --tcp-keepalive 300
      --timeout 300
      --databases 4
      --hash-max-ziplist-entries 512
      --hash-max-ziplist-value 64
      --list-max-ziplist-size -2
      --set-max-intset-entries 512
      --zset-max-ziplist-entries 128
      --zset-max-ziplist-value 64
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    deploy:
      resources:
        limits:
          memory: 96M  # Ultra-strict Redis limit
          cpus: '0.25'
        reservations:
          memory: 48M
          cpus: '0.1'
    networks:
      - backend-network
      - monitoring-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 3s
      retries: 3

  # PostgreSQL - Memory Tuned
  postgres:
    image: postgres:16-alpine
    container_name: sixty-postgres-optimized
    environment:
      - POSTGRES_DB=sixty_sales_dashboard
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      # Memory-optimized PostgreSQL settings
      - POSTGRES_SHARED_BUFFERS=96MB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=256MB
      - POSTGRES_WORK_MEM=2MB
      - POSTGRES_MAINTENANCE_WORK_MEM=32MB
      - POSTGRES_WAL_BUFFERS=8MB
      - POSTGRES_CHECKPOINT_COMPLETION_TARGET=0.9
      - POSTGRES_DEFAULT_STATISTICS_TARGET=50
      - POSTGRES_RANDOM_PAGE_COST=1.1
      - POSTGRES_EFFECTIVE_IO_CONCURRENCY=100
      - POSTGRES_MAX_CONNECTIONS=50
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./backups:/backups
    deploy:
      resources:
        limits:
          memory: 512M  # Controlled database memory
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.5'
    networks:
      - backend-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d sixty_sales_dashboard"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Monitoring - Prometheus (Lightweight)
  prometheus:
    image: prom/prometheus:v2.47.2
    container_name: sixty-prometheus-optimized
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'  # Reduced retention
      - '--storage.tsdb.retention.size=1GB'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--query.max-concurrency=10'
      - '--storage.tsdb.min-block-duration=2h'
      - '--storage.tsdb.max-block-duration=2h'
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus-memory-optimized.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    deploy:
      resources:
        limits:
          memory: 128M  # Constrained Prometheus
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.1'
    networks:
      - monitoring-network

  # Monitoring - Grafana (Minimal)
  grafana:
    image: grafana/grafana:10.2.0
    container_name: sixty-grafana-optimized
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_SERVER_ROOT_URL=https://monitoring.yourdomain.com
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_INSTALL_PLUGINS=
      - GF_FEATURE_TOGGLES_ENABLE=
      - GF_DATABASE_WAL=false
      - GF_METRICS_ENABLED=false
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    deploy:
      resources:
        limits:
          memory: 128M  # Minimal Grafana
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.1'
    networks:
      - monitoring-network
    depends_on:
      - prometheus

  # Memory Monitor - Custom Service
  memory-monitor:
    build:
      context: ./monitoring/memory-monitor
      dockerfile: Dockerfile
    container_name: sixty-memory-monitor
    environment:
      - MEMORY_THRESHOLD_WARNING=80
      - MEMORY_THRESHOLD_CRITICAL=90
      - CHECK_INTERVAL=30
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}
      - ALERT_EMAIL=${ALERT_EMAIL}
    volumes:
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
      - /proc:/proc:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    deploy:
      resources:
        limits:
          memory: 32M
          cpus: '0.1'
        reservations:
          memory: 16M
          cpus: '0.05'
    networks:
      - monitoring-network
    privileged: true

  # Log Management - Lightweight
  loki:
    image: grafana/loki:2.9.2
    container_name: sixty-loki-optimized
    command: 
      - -config.file=/etc/loki/local-config.yaml
    ports:
      - "3100:3100"
    volumes:
      - ./config/loki-memory-optimized.yml:/etc/loki/local-config.yaml:ro
      - loki-data:/loki
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.1'
    networks:
      - monitoring-network

  # Performance Testing Service
  performance-tester:
    build:
      context: ./monitoring/performance-tester
      dockerfile: Dockerfile
    container_name: sixty-performance-tester
    environment:
      - TARGET_URL=http://frontend
      - TEST_INTERVAL=300  # 5 minutes
      - MEMORY_THRESHOLD=80
      - RESPONSE_TIME_THRESHOLD=2000
    volumes:
      - ./performance-results:/app/results
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: '0.1'
        reservations:
          memory: 32M
          cpus: '0.05'
      mode: replicated
      replicas: 0  # Only run when needed
    networks:
      - monitoring-network
      - frontend-network
    profiles:
      - testing

networks:
  frontend-network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: sixty-frontend
  backend-network:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.name: sixty-backend
  monitoring-network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: sixty-monitoring

volumes:
  # Data persistence
  postgres-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/postgres
  redis-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/redis
  
  # Monitoring data
  prometheus-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/prometheus
  grafana-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/grafana
  loki-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/loki
  
  # Cache volumes (tmpfs for performance)
  nginx-cache-api:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: "size=64m,uid=101,gid=101,mode=755"
  nginx-cache-static:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: "size=128m,uid=101,gid=101,mode=755"

# Production deployment configuration
x-deploy-config: &deploy-config
  placement:
    constraints:
      - node.role == manager
  update_config:
    parallelism: 1
    delay: 10s
    failure_action: rollback
    monitor: 60s
    max_failure_ratio: 0.3
  rollback_config:
    parallelism: 1
    delay: 0s
    failure_action: pause
    monitor: 60s
  restart_policy:
    condition: unless-stopped
    delay: 5s
    max_attempts: 3
    window: 120s

# Memory optimization labels
labels:
  - "project=sixty-sales-dashboard"
  - "environment=production"
  - "memory-optimized=true"
  - "version=${VERSION:-latest}"